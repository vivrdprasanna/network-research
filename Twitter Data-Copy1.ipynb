{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afe432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a9ef0",
   "metadata": {},
   "source": [
    "# Importing & Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9200bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv('edgelist.csv')\n",
    "hateful_user_df = pd.read_csv('hateful_users.csv')\n",
    "\n",
    "# Filter out rows where src and dst are the same\n",
    "filtered_twitter_df = twitter_df[twitter_df['src'] != twitter_df['dst']]\n",
    "\n",
    "# Identify the excluded user IDs in twitter_df\n",
    "excluded_ids = set(twitter_df['src']) - set(filtered_twitter_df['src'])\n",
    "excluded_ids.update(set(twitter_df['dst']) - set(filtered_twitter_df['dst']))\n",
    "\n",
    "# Filter out these user IDs from hateful_user_df\n",
    "filtered_hateful_user_df = hateful_user_df[~hateful_user_df['user_id'].isin(excluded_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e39d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>producer</th>\n",
       "      <th>consumer</th>\n",
       "      <th>outdegree</th>\n",
       "      <th>indegree</th>\n",
       "      <th>degree</th>\n",
       "      <th>outstrength</th>\n",
       "      <th>instrength</th>\n",
       "      <th>strength</th>\n",
       "      <th>n_tweets</th>\n",
       "      <th>n_hate_tweets</th>\n",
       "      <th>n_northerner_tweets</th>\n",
       "      <th>n_hausa_tweets</th>\n",
       "      <th>n_fulani_tweets</th>\n",
       "      <th>n_southerner_tweets</th>\n",
       "      <th>n_yoruba_tweets</th>\n",
       "      <th>n_biafra_tweets</th>\n",
       "      <th>n_igbo_tweets</th>\n",
       "      <th>n_retweets</th>\n",
       "      <th>n_hate_retweets</th>\n",
       "      <th>n_northerner_retweets</th>\n",
       "      <th>n_hausa_retweets</th>\n",
       "      <th>n_fulani_retweets</th>\n",
       "      <th>n_southerner_retweets</th>\n",
       "      <th>n_yoruba_retweets</th>\n",
       "      <th>n_biafra_retweets</th>\n",
       "      <th>n_igbo_retweets</th>\n",
       "      <th>n_likes</th>\n",
       "      <th>n_hate_likes</th>\n",
       "      <th>n_northerner_likes</th>\n",
       "      <th>n_hausa_likes</th>\n",
       "      <th>n_fulani_likes</th>\n",
       "      <th>n_southerner_likes</th>\n",
       "      <th>n_yoruba_likes</th>\n",
       "      <th>n_biafra_likes</th>\n",
       "      <th>n_igbo_likes</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>religion</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_timestamp</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_favourites_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>sampling_proportion_likes</th>\n",
       "      <th>sampling_proportion_timelines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1016</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.162282e+09</td>\n",
       "      <td>259.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>32294.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.998249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2340131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1141</td>\n",
       "      <td>5</td>\n",
       "      <td>1008</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1546.0</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>igbo</td>\n",
       "      <td>christian</td>\n",
       "      <td>m</td>\n",
       "      <td>1.174936e+09</td>\n",
       "      <td>5301.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.984909</td>\n",
       "      <td>0.998249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3741461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3139</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.175985e+09</td>\n",
       "      <td>637.0</td>\n",
       "      <td>742.0</td>\n",
       "      <td>5782.0</td>\n",
       "      <td>11655.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.951123</td>\n",
       "      <td>0.998249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  producer  consumer  outdegree  indegree  degree  outstrength  \\\n",
       "0    11172       1.0       1.0          7        11      16         11.0   \n",
       "1  2340131       1.0       1.0       1141         5    1008       1541.0   \n",
       "2  3741461       1.0       1.0          2         2       2         13.0   \n",
       "\n",
       "   instrength  strength  n_tweets  n_hate_tweets  n_northerner_tweets  \\\n",
       "0        98.0     109.0         6              0                    0   \n",
       "1         5.0    1546.0       106              3                    1   \n",
       "2         3.0      16.0       100              0                    0   \n",
       "\n",
       "   n_hausa_tweets  n_fulani_tweets  n_southerner_tweets  n_yoruba_tweets  \\\n",
       "0               0                0                    0                0   \n",
       "1               0                2                    0                0   \n",
       "2               0                0                    0                0   \n",
       "\n",
       "   n_biafra_tweets  n_igbo_tweets  n_retweets  n_hate_retweets  \\\n",
       "0                0              0           1                0   \n",
       "1                0              0          78                0   \n",
       "2                0              0         812                0   \n",
       "\n",
       "   n_northerner_retweets  n_hausa_retweets  n_fulani_retweets  \\\n",
       "0                      0                 0                  0   \n",
       "1                      0                 0                  0   \n",
       "2                      0                 0                  0   \n",
       "\n",
       "   n_southerner_retweets  n_yoruba_retweets  n_biafra_retweets  \\\n",
       "0                      0                  0                  0   \n",
       "1                      0                  0                  0   \n",
       "2                      0                  0                  0   \n",
       "\n",
       "   n_igbo_retweets  n_likes  n_hate_likes  n_northerner_likes  n_hausa_likes  \\\n",
       "0                0     1016            10                  10              0   \n",
       "1                0       31             0                   0              0   \n",
       "2                0     3139             2                   2              0   \n",
       "\n",
       "   n_fulani_likes  n_southerner_likes  n_yoruba_likes  n_biafra_likes  \\\n",
       "0               0                   0               0               0   \n",
       "1               0                   0               0               0   \n",
       "2               0                   0               0               0   \n",
       "\n",
       "   n_igbo_likes ethnicity   religion gender  user_timestamp  \\\n",
       "0             0       NaN        NaN    NaN    1.162282e+09   \n",
       "1             0      igbo  christian      m    1.174936e+09   \n",
       "2             0       NaN        NaN    NaN    1.175985e+09   \n",
       "\n",
       "   user_followers_count  user_friends_count  user_statuses_count  \\\n",
       "0                 259.0               334.0               2544.0   \n",
       "1                5301.0               665.0               1274.0   \n",
       "2                 637.0               742.0               5782.0   \n",
       "\n",
       "   user_favourites_count  user_verified  sampling_proportion_likes  \\\n",
       "0                32294.0          False                   0.029260   \n",
       "1                  162.0          False                   0.984909   \n",
       "2                11655.0          False                   0.951123   \n",
       "\n",
       "   sampling_proportion_timelines  \n",
       "0                       0.998249  \n",
       "1                       0.998249  \n",
       "2                       0.998249  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_hateful_user_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7cff4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dst</th>\n",
       "      <th>src</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>n_retweets</th>\n",
       "      <th>n_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642207375</td>\n",
       "      <td>1304915551467208707</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>703569617</td>\n",
       "      <td>985075589995524096</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1331088135825010690</td>\n",
       "      <td>1343310549061406720</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dst                  src  n_mentions  n_retweets  n_tweets\n",
       "0           1642207375  1304915551467208707         4.0         4.0       4.0\n",
       "1            703569617   985075589995524096         2.0         0.0       2.0\n",
       "4  1331088135825010690  1343310549061406720         5.0         0.0       5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_twitter_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f97d6",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d938ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nodes_with_few_connections(subgraph, min_degree=4):\n",
    "    # Nodes to be removed\n",
    "    nodes_to_remove = [node for node in subgraph.nodes if subgraph.degree(node) < min_degree]\n",
    "\n",
    "    # Remove these nodes from the subgraph\n",
    "    subgraph.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "    return subgraph\n",
    "\n",
    "def get_random_subset(G, num_nodes):\n",
    "    # Ensure that the number of nodes requested is not more than the number of nodes in the graph\n",
    "    num_nodes = min(num_nodes, len(G.nodes))\n",
    "\n",
    "    # Select a random subset of nodes\n",
    "    random_nodes = random.sample(G.nodes(), num_nodes)\n",
    "\n",
    "    # Create a subgraph with the selected nodes\n",
    "    random_subset_graph = G.subgraph(random_nodes).copy()\n",
    "    return random_subset_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c924be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph_spring(G, cat_df, attribute_name):\n",
    "    # Get the values of the category\n",
    "    cat_values = cat_df[cat_df.index.isin(G.nodes)][attribute_name].unique()\n",
    "    colors = [plt.cm.Set2(i/float(len(cat_values)-1)) for i in range(len(cat_values))]\n",
    "\n",
    "    # Create a dictionary mapping category values to colors & map nodes to colors\n",
    "    value_to_color = dict(zip(cat_values, colors))\n",
    "    node_colors = [value_to_color[cat_df.loc[n][attribute_name]] for n in G.nodes() if n in cat_df.index]\n",
    "\n",
    "    # Calculate the layout positions using the spring layout\n",
    "    positions = nx.spring_layout(G)\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    nx.draw(G, pos=positions, node_size=25, node_color=node_colors, with_labels=False)\n",
    "\n",
    "    # Create a patch for each category value for the legend\n",
    "    patches = [plt.plot([],[], marker=\"o\", color=color, \n",
    "                        label=str(value), linestyle='None')[0] for value, color in value_to_color.items()]\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(handles=patches, title=attribute_name)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cead8dd",
   "metadata": {},
   "source": [
    "### Clustering by `ethnicity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85739728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connected_subset_by_ethnicity(G, num_nodes):\n",
    "    # Ensure at least 5% nodes for each ethnicity\n",
    "    min_nodes_per_ethnicity = int(num_nodes * 0.05)\n",
    "\n",
    "    # Filter nodes by ethnicity\n",
    "    nodes_igbo = [node for node, attr in G.nodes(data=True) if attr.get('ethnicity') == 'igbo']\n",
    "    nodes_yoruba = [node for node, attr in G.nodes(data=True) if attr.get('ethnicity') == 'yoruba']\n",
    "    nodes_hausa = [node for node, attr in G.nodes(data=True) if attr.get('ethnicity') == 'hausa']\n",
    "\n",
    "    # Calculate the number of nodes for each ethnicity\n",
    "    num_igbo = max(min_nodes_per_ethnicity, len(nodes_igbo))\n",
    "    num_yoruba = max(min_nodes_per_ethnicity, len(nodes_yoruba))\n",
    "    num_hausa = max(min_nodes_per_ethnicity, len(nodes_hausa))\n",
    "\n",
    "    # Adjust the numbers if the total exceeds num_nodes\n",
    "    total_num = num_igbo + num_yoruba + num_hausa\n",
    "    while total_num > num_nodes:\n",
    "        # Reduce the count of the largest group\n",
    "        if num_igbo > num_yoruba and num_igbo > num_hausa:\n",
    "            num_igbo -= 1\n",
    "        elif num_yoruba > num_igbo and num_yoruba > num_hausa:\n",
    "            num_yoruba -= 1\n",
    "        else:\n",
    "            num_hausa -= 1\n",
    "        total_num = num_igbo + num_yoruba + num_hausa\n",
    "\n",
    "    # Get connected components for each ethnicity\n",
    "    components_igbo = [G.subgraph(c).copy() for c in nx.connected_components(G.subgraph(nodes_igbo))]\n",
    "    components_yoruba = [G.subgraph(c).copy() for c in nx.connected_components(G.subgraph(nodes_yoruba))]\n",
    "    components_hausa = [G.subgraph(c).copy() for c in nx.connected_components(G.subgraph(nodes_hausa))]\n",
    "\n",
    "    # Sort connected components by size\n",
    "    components_igbo.sort(key=len, reverse=True)\n",
    "    components_yoruba.sort(key=len, reverse=True)\n",
    "    components_hausa.sort(key=len, reverse=True)\n",
    "\n",
    "    # Select the largest connected components for each ethnicity\n",
    "    largest_cc_igbo = components_igbo[0] if components_igbo else None\n",
    "    largest_cc_yoruba = components_yoruba[0] if components_yoruba else None\n",
    "    largest_cc_hausa = components_hausa[0] if components_hausa else None\n",
    "\n",
    "    # Select top nodes by degree within each ethnicity\n",
    "    selected_nodes_igbo = sorted(largest_cc_igbo.nodes, key=lambda node: G.degree(node), reverse=True)[:num_igbo] if largest_cc_igbo else []\n",
    "    selected_nodes_yoruba = sorted(largest_cc_yoruba.nodes, key=lambda node: G.degree(node), reverse=True)[:num_yoruba] if largest_cc_yoruba else []\n",
    "    selected_nodes_hausa = sorted(largest_cc_hausa.nodes, key=lambda node: G.degree(node), reverse=True)[:num_hausa] if largest_cc_hausa else []\n",
    "\n",
    "    # Create a subgraph with the selected nodes\n",
    "    selected_nodes = selected_nodes_igbo + selected_nodes_yoruba + selected_nodes_hausa\n",
    "    subset_graph = G.subgraph(selected_nodes)\n",
    "    largest_cc = max(nx.connected_components(subset_graph), key=len, default=set())\n",
    "    subset_graph = subset_graph.subgraph(largest_cc).copy()\n",
    "\n",
    "    return subset_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hateful_user_df_ethnicity_na_dropped = filtered_hateful_user_df.dropna(subset=['ethnicity'])\n",
    "cat_df = filtered_hateful_user_df_ethnicity_na_dropped.set_index('user_id')['ethnicity'].to_frame()\n",
    "\n",
    "# Rename the column\n",
    "cat_df.columns = ['ethnicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in twitter_df where either 'src' or 'dst' is not in the list of user IDs with non-missing 'ethnicity'\n",
    "valid_user_ids = set(filtered_hateful_user_df_ethnicity_na_dropped['user_id'])\n",
    "filtered_edges = filtered_twitter_df[\n",
    "    filtered_twitter_df['src'].isin(valid_user_ids) & \n",
    "    filtered_twitter_df['dst'].isin(valid_user_ids)\n",
    "]\n",
    "\n",
    "# Now create the graph\n",
    "G = nx.from_pandas_edgelist(filtered_edges, 'src', 'dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in G.nodes:\n",
    "    G.nodes[node]['ethnicity'] = cat_df.get(node, None)\n",
    "\n",
    "G_random_subset = get_random_subset(G, 2000)\n",
    "G_random_subset = remove_nodes_with_few_connections(G_random_subset, min_degree=3)\n",
    "draw_graph_spring(G_random_subset, cat_df, 'ethnicity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7334a82",
   "metadata": {},
   "source": [
    "### Clustering by `n_tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252ea94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connected_subset_by_tweet_category(G, num_nodes):\n",
    "    # Minimum representation for each category\n",
    "    min_rep = int(num_nodes * 0.1)\n",
    "\n",
    "    # Filter nodes by tweet category\n",
    "    category_nodes = defaultdict(list)\n",
    "    for node, data in G.nodes(data=True):\n",
    "        category = data.get('tweet_category')\n",
    "        category_nodes[category].append(node)\n",
    "\n",
    "    # Ensure at least 10% nodes for each category\n",
    "    for category in category_nodes:\n",
    "        category_nodes[category] = random.sample(category_nodes[category], \n",
    "                                                 max(min_rep, len(category_nodes[category])))\n",
    "\n",
    "    # Start from a random node\n",
    "    start_node = random.choice(list(G.nodes()))\n",
    "    visited = set([start_node])\n",
    "    queue = [start_node]\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    with tqdm(total=num_nodes, desc=\"Building Subset\") as pbar:\n",
    "        while len(visited) < num_nodes and queue:\n",
    "            current = queue.pop(0)\n",
    "            neighbors = [n for n in G.neighbors(current) if n not in visited]\n",
    "\n",
    "            # Randomize neighbors\n",
    "            random.shuffle(neighbors)\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                # Add neighbor if it maintains the 10% representation\n",
    "                neighbor_category = G.nodes[neighbor].get('tweet_category')\n",
    "                if len([n for n in visited if G.nodes[n].get('tweet_category') == neighbor_category]) < min_rep or \\\n",
    "                   len(visited) + len(queue) < num_nodes:\n",
    "                    visited.add(neighbor)\n",
    "                    queue.append(neighbor)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                if len(visited) >= num_nodes:\n",
    "                    break\n",
    "\n",
    "    # Create the subgraph\n",
    "    subset_graph = G.subgraph(visited).copy()\n",
    "\n",
    "    return subset_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "445e6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hateful_user_df['tweet_category'], bins = pd.qcut(filtered_hateful_user_df['n_tweets'], 3, labels=['low', 'medium', 'high'], retbins=True)\n",
    "cat_df = filtered_hateful_user_df[['user_id', 'tweet_category']].set_index('user_id')\n",
    "\n",
    "# Create a set of user IDs from the filtered_hateful_user_df\n",
    "hateful_users_set = set(filtered_hateful_user_df['user_id'])\n",
    "\n",
    "# Filter the filtered_twitter_df to keep only rows where both src and dst are in hateful_users_set\n",
    "filtered_twitter_df = filtered_twitter_df[\n",
    "    filtered_twitter_df['src'].isin(hateful_users_set) & \n",
    "    filtered_twitter_df['dst'].isin(hateful_users_set)\n",
    "]\n",
    "\n",
    "G = nx.from_pandas_edgelist(filtered_twitter_df, 'src', 'dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbee6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77221, 77247)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes), len(hateful_users_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5912e825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1590c60ecf2445ac9a45b5c5cb2daefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning Categories:   0%|          | 0/77221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for node in tqdm(G.nodes, desc='Assigning Categories'):\n",
    "    node_category = cat_df.loc[node, 'tweet_category'] if node in cat_df.index else None\n",
    "    G.nodes[node]['tweet_category'] = node_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7fc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5039183622479b9736e48a559c4b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Subset:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G_subset = get_connected_subset_by_tweet_category(G, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph_spring(G_subset, cat_df, 'tweet_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "081abeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of all unique user IDs in filtered_twitter_df (both src and dst)\n",
    "twitter_users_set = set(filtered_twitter_df['src']).union(set(filtered_twitter_df['dst']))\n",
    "\n",
    "# Find users in hateful_users_set who are not in twitter_users_set\n",
    "isolated_users = hateful_users_set - twitter_users_set\n",
    "\n",
    "# Count of isolated users\n",
    "len(isolated_users) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
