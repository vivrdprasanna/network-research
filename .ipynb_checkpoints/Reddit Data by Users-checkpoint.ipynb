{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8afe432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import networkx as nx\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e35ef",
   "metadata": {},
   "source": [
    "### Collecting Data\n",
    "- users ~ nodes\n",
    "- userA replies to userB's post: directed(?) edge.\n",
    "- user's attribute: subreddit they are most active in\n",
    "    - \"most active\" ~ \"most number of comments\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6e5cd",
   "metadata": {},
   "source": [
    "Initial Node Identification: You begin by selecting a popular Reddit post and using its author as the starting node in your network.\n",
    "\n",
    "First-Level Reply Collection: For this initial post, you collect all first-level replies, establishing direct connections (edges) between the original poster and the users who replied.\n",
    "\n",
    "User Community Determination: For each user (node) in the network, you determine their most active subreddit by analyzing their recent comments and identifying the subreddit where they are most frequently active.\n",
    "\n",
    "Network Expansion: Utilizing a breadth-first search (BFS) method, you systematically expand the network. For each new user added, you repeat the process of collecting first-level replies and identifying their most active subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93087d3",
   "metadata": {},
   "source": [
    "_Alternative:_\n",
    "\n",
    "I start by identifying the top 100 most upvoted users on the platform. \n",
    "\n",
    "My approach is to employ a depth-first search (DFS) method for each of these top users, analyzing their comments in detail. I track all their interactions, focusing on both users who have replied to them and those they have replied to. As I delve into the comment chains, every new user I encounter triggers the same DFS process, leading to a systematic expansion of the network. \n",
    "\n",
    "To manage the scope of this expansion, I limit the process to either 1000 iterations or until I detect a cycle in these interactions. This method is designed to create a detailed map of user interactions and relationships on Reddit, centered around its most influential members.\n",
    "\n",
    "- There's a possibility that the top upvoted users may not be representative of the broader Reddit community. Their interactions could be skewed towards certain topics or subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb057ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Reddit connection with API credentials\n",
    "def initialize_reddit():\n",
    "    # Reddit API credentials\n",
    "    client_id = '9zk3ptJvrNihAEGiiyTYPg'\n",
    "    client_secret = 'dpYAGYqjV3jfcdg7bNjXJ9gX99cOuQ'\n",
    "    user_agent = 'network analysis'\n",
    "    username = \"stuffingmybrain\"\n",
    "    return praw.Reddit(client_id=client_id,\n",
    "                       client_secret=client_secret,\n",
    "                       user_agent=user_agent,\n",
    "                       username=username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa24a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the first-level replies to a Reddit submission\n",
    "def fetch_first_level_replies(submission):\n",
    "    first_level_replies = []\n",
    "    try:\n",
    "        # Expanding the comments and filtering out non-first-level replies\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list():\n",
    "            if comment.parent_id == submission.fullname:\n",
    "                first_level_replies.append(comment)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments: {e}\")\n",
    "    return first_level_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01724786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the most active subreddit for a given Reddit user\n",
    "def compute_most_active_subreddit(reddit, username):\n",
    "    subreddit_activity = defaultdict(int)\n",
    "    try:\n",
    "        # Iterating through recent comments of the user and counting subreddit activity\n",
    "        for comment in reddit.redditor(username).comments.new(limit=1000):\n",
    "            subreddit_activity[comment.subreddit.display_name] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching user comments: {e}\")\n",
    "    return max(subreddit_activity, key=subreddit_activity.get, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b71420d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berkeley'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = initialize_reddit()\n",
    "compute_most_active_subreddit(reddit, \"stuffingmybrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b17b3",
   "metadata": {},
   "source": [
    "### Constructing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea86d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the network using BFS starting from a root submission\n",
    "def bfs_network_expansion(reddit, root_submission):\n",
    "    visited = set()\n",
    "    queue = deque([root_submission])\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    while queue:\n",
    "        current_submission = queue.popleft()\n",
    "        try:\n",
    "            # Getting the author of the current submission\n",
    "            author_name = current_submission.author.name if current_submission.author else \"Deleted\"\n",
    "        except Exception:\n",
    "            author_name = \"Deleted\"\n",
    "        replies = fetch_first_level_replies(current_submission)\n",
    "        for reply in replies:\n",
    "            replier_name = reply.author.name if reply.author else \"Deleted\"\n",
    "            if replier_name not in visited:\n",
    "                visited.add(replier_name)\n",
    "                most_active_subreddit = compute_most_active_subreddit(reddit, replier_name)\n",
    "                if most_active_subreddit:\n",
    "                    G.add_node(replier_name, subreddit=most_active_subreddit)\n",
    "                if author_name != \"Deleted\":\n",
    "                    G.add_edge(author_name, replier_name)\n",
    "                # Add additional logic here if I wish to queue further levels\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = initialize_reddit()\n",
    "submission_url = 'https://www.reddit.com/r/...'  # Replace with the actual URL of the desired post\n",
    "submission = reddit.submission(url=submission_url)\n",
    "network = bfs_network_expansion(reddit, submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
